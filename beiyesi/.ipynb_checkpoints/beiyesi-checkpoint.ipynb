{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec\n",
    "#构件词汇表\n",
    "def ctreateVocaList(dataset):\n",
    "    vocaSet = set([])\n",
    "    for document in dataset:\n",
    "        vocaSet = vocaSet | set(document)\n",
    "    return list(vocaSet)   #set转list\n",
    "\n",
    "#词向量\n",
    "def setOfWordsVec(vocSet,inputSet):#构建词向量\n",
    "    returnVec = [0] * len(vocSet)\n",
    "    for word in inputSet:\n",
    "        if word in vocSet:\n",
    "            returnVec[vocSet.index(word)] = 1\n",
    "    return returnVec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0]\n",
      "['not', 'quit', 'ate', 'has', 'so', 'steak', 'food', 'love', 'dog', 'licks', 'posting', 'worthless', 'stupid', 'how', 'cute', 'problems', 'help', 'maybe', 'park', 'is', 'I', 'dalmation', 'him', 'garbage', 'buying', 'mr', 'flea', 'stop', 'please', 'to', 'my', 'take']\n"
     ]
    }
   ],
   "source": [
    "postingList,classVec = loadDataSet()\n",
    "vocSet = ctreateVocaList(postingList)\n",
    "trainVec = []\n",
    "for docu in postingList:\n",
    "    trainVec.append(setOfWordsVec(vocSet,docu))\n",
    "print(trainVec[0])\n",
    "print(vocSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.04452244 -3.04452244        -inf        -inf        -inf        -inf\n",
      " -3.04452244        -inf -2.35137526        -inf -3.04452244 -2.35137526\n",
      " -1.94591015        -inf        -inf        -inf        -inf -3.04452244\n",
      " -3.04452244        -inf        -inf        -inf -3.04452244 -3.04452244\n",
      " -3.04452244        -inf        -inf -3.04452244        -inf -3.04452244\n",
      "        -inf -3.04452244]\n",
      "-1.9459101490553135\n",
      "stupid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def NaiveBayes(trainVec,trainCatory):\n",
    "    numTrain = len(trainVec)   #行数\n",
    "    numWords = len(trainVec[0])   #词汇表的大小、即特征的大小\n",
    "    #分类1   p(y = 1)   的概率，这里是2分类，标签是0或1，所以sum(trainCatory)就是1的个数   p0 = 1 - p1\n",
    "    p1 = sum(trainCatory) / float(numTrain)   \n",
    "    p0Num = np.zeros(numWords)\n",
    "    p1Num = np.zeros(numWords)\n",
    "    p0Denom = 2.0;p1Denom = 2.0   #有些概率=0，那么，后面的概率相乘，会影响计算结果，所以改进\n",
    "    for i in range(numTrain):\n",
    "        if trainCatory[i] == 1:\n",
    "            p1Num += trainVec[i]  #矩阵相加  统计y = 1条件下，统计某个单词出现的个数-> p(w[i]/y = 1)\n",
    "            p1Denom += sum(trainVec[i])  #累计y=1的所有单词数量\n",
    "        else:\n",
    "            p0Num += trainVec[i]  #矩阵相加  统计y = 1条件下，统计某个单词出现的个数-> p(w[i]/y = 0)\n",
    "            p0Num += sum(trainVec[i])  #累计y=1的所有单词数量\n",
    "    p1Vect = np.log(p1Num / float(p1Denom))   #很多小数相乘，会下溢出，所以加上log   以e为底\n",
    "    p0Vect = np.log(p0Num / float(p0Denom))\n",
    "    \n",
    "    return p0Vect,p1Vect,p1\n",
    "#测试\n",
    "p0Vect,p1Vect,p1 = NaiveBayes(trainVec,classVec) \n",
    "print(p1Vect)\n",
    "print(np.max(p1Vect))\n",
    "print(vocSet[np.argmax(p1Vect)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classfy(testVec,p1Vec,p0Vec,pClass1):\n",
    "    #p(y=1/w) = p(w/y=1) * p(y=1)    ln(a*b) = ln(a) + ln(b)\n",
    "    p1 = sum(testVec * p1Vec) + np.log(pClass1)\n",
    "    p0 = sum(testVec * p0Vec) + np.log(1 - pClass1)\n",
    "    if p1 > p0:\n",
    "        return 1\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in multiply\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test = ['love','you','stupid']\n",
    "testVec = setOfWordsVec(vocSet,test)\n",
    "res = classfy(testVec,p1Vect,p0Vect,p1)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "def ReadData(path):\n",
    "    wordlists = []\n",
    "    files = os.listdir(path)\n",
    "    for file in files:\n",
    "        wordlist = []\n",
    "        filename = os.path.join(path,file)\n",
    "        f = open(filename,encoding='ISO-8859-1')\n",
    "        line = f.read()\n",
    "        tokens = re.split('\\W*',line)\n",
    "        for token in tokens:\n",
    "            if len(token) > 2:\n",
    "                wordlist.append(token)\n",
    "        wordlists.append(wordlist)\n",
    "    return wordlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Instead', 'web', 'Jar', 'parallel', 'York', 'Team', 'reply', 'brained', 'CUDA', 'advocate', 'can', 'thank', 'about', 'want', 'required', 'forum', 'customized', 'longer', 'when', 'jpgs', 'aged', 'Docs', 'past', 'How', 'These', 'announcement', 'more', 'logged', 'Zach', 'made', 'all', 'pick', 'the', 'retirement', 'The', 'Series', 'strategic', 'focus', 'Design', 'comment', 'website', 'hold', 'lined', 'enough', 'wednesday', 'need', 'cold', 'came', 'shape', 'held', 'status', 'keep', 'see', 'sliding', 'important', 'winter', 'Vivek', 'thought', 'might', 'supporting', 'Let', 'ready', 'management', 'town', 'Sorry', '14th', 'mail', 'computing', 'WHat', '2010', 'prepared', 'Regards', 'tour', 'far', 'least', 'CCA', 'done', 'modelling', 'sophisticated', 'doing', 'heard', 'Troy', 'knew', '100M', 'station', 'requested', 'group', 'free', 'new', 'glimpse', 'style', 'enabled', 'windows', 'Are', 'link', 'bathroom', 'through', 'too', 'please', 'decision', 'for', 'not', 'risk', 'runs', 'also', 'here', 'lists', 'Magazine', 'automatically', 'expo', 'FBI', 'same', 'stuff', 'coast', 'job', 'color', 'cannot', 'owner', 'faster', 'insights', 'top', 'class', 'October', 'questions', 'riding', 'you', 'placed', 'Germany', 'online', 'Groups', 'For', 'articles', 'father', 'fractal', 'tent', 'ideas', '50092', 'Yay', 'close', 'upload', 'Does', 'model', 'died', 'dozen', 'rent', 'Just', 'been', '174623', 'download', 'contact', 'Giants', 'cats', 'another', 'Carlo', 'once', 'functionalities', 'hotel', 'must', 'favorite', 'concise', 'get', 'could', 'borders', 'You', 'going', 'take', 'sent', 'those', 'fans', 'sky', 'food', 'significantly', 'Sure', 'try', 'interesting', 'Python', 'connection', 'scenic', 'hangzhou', 'inconvenience', 'looking', 'away', 'there', 'Since', 'generation', 'Wilmott', 'being', '86152', 'time', 'place', 'inside', 'example', 'level', 'Strategy', 'features', 'Tesla', 'think', 'easily', 'check', 'book', 'They', 'foaming', 'programming', 'individual', 'trip', 'issues', 'but', 'Sites', 'don', 'girl', 'will', 'others', 'items', 'Peter', 'possible', 'discussions', 'latest', 'received', 'LinkedIn', 'using', 'answer', 'thread', 'Kerry', 'china', 'invitation', 'hope', 'com', 'information', 'assigning', 'Where', '2011', 'used', 'would', 'Can', '300x', 'item', 'name', 'inform', 'then', 'Looking', 'good', 'survive', 'way', 'address', 'accept', 'There', 'focusing', 'care', 'what', 'often', 'game', 'products', 'spaying', 'starting', 'well', 'them', 'expertise', 'because', 'car', 'follow', 'jquery', 'party', 'listed', 'mandatory', 'yeah', 'Take', 'these', 'With', 'reservation', '90563', 'file', 'exhibit', 'train', 'roofer', 'sites', 'pictures', '66343', 'rain', 'site', 'should', 'mathematician', 'number', 'huge', 'blue', 'writing', 'located', 'Jocelyn', 'plane', 'John', 'signed', 'This', 'Will', 'nature', 'mom', 'day', 'members', 'designed', 'while', 'Monte', 'share', 'than', 'creative', 'edit', 'assistance', 'hotels', 'saw', 'bike', 'fine', 'commented', 'horn', 'create', 'Arvind', 'google', 'meet', 'MBA', 'Benoit', 'ferguson', 'mathematics', 'enjoy', 'store', 'www', 'Google', 'Hommies', 'prices', 'Sounds', 'phone', 'museum', 'Thirumalai', 'February', 'holiday', 'and', 'http', 'went', 'door', 'Perhaps', 'support', 'thing', 'out', 'only', 'told', 'specifically', 'New', 'note', 'pavilion', 'changing', 'your', 'improving', 'one', 'page', 'view', 'school', 'Thank', 'cat', 'this', 'Stepp', 'Jay', 'his', 'wrote', 'such', 'talked', 'derivatives', 'like', 'NVIDIA', 'couple', 'copy', 'Jose', 'Could', 'creation', 'bad', 'SciFinance', 'Chinese', 'window', 'update', 'butt', 'Mandarin', 'pages', 'welcome', 'bin', 'help', 'finance', 'know', 'are', 'plugin', 'much', 'files', 'add', 'suggest', 'storage', 'service', 'got', 'Fermi', 'access', 'they', 'prototype', 'computer', 'Eugene', 'work', 'below', 'approach', 'has', 'running', 'jqplot', 'groups', 'Cheers', 'notification', 'capabilities', 'night', 'serial', 'fundamental', 'definitely', 'Mandelbrot', 'includes', 'View', 'professional', 'some', 'ones', 'launch', 'art', 'Please', 'uses', 'generates', 'Hamm', 'specifications', 'just', 'each', 'said', 'had', 'leaves', 'gas', 'now', 'guy', 'doors', 'Hello', 'grounds', 'Thailand', 'turd', 'Haloney', 'differ', 'from', 'was', 'Whybrew', 'drunk', 'come', 'development', '1924', 'selected', 'forward', 'call', 'pricing', 'code', 'two', 'Accept', 'chapter', 'that', 'quantitative', 'encourage', 'today', 'That', 'working', 'dusty', 'email', 'Ryan', 'featured', 'source', 'location', 'tickets', 'wasn', 'attaching', 'Thanks', 'behind', 'program', 'high', 'magazine', 'release', 'yesterday', 'message', 'may', 'back', 'hours', 'core', 'GPU', 'Year', 'StoreDetailView_98', 'Reply', 'who', 'pls', 'either', 'right', 'with', 'changes', 'automatic', 'any', 'have', 'tool', 'docs', 'Julius', 'network', 'how', 'doggy', 'extended', 'Tokyo', 'mailing', 'province', 'having', 'inspired', 'use', 'things', 'both', 'incoming', 'lunch', 'pretty']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "lists = ReadData('/Users/yangyang/Desktop/github/Deep-Learning/beiyesi/data')\n",
    "vocabularyList = ctreateVocaList(lists)\n",
    "print(vocabularyList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
